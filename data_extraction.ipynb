{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47bce407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Database Connection Configuration ---\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'root', \n",
    "    'password': 'sandeep', \n",
    "    'database': 'phonepe_pulse_db'\n",
    "}\n",
    "\n",
    "# --- Path to my cloned 'pulse' repository data ---\n",
    "# This path points to the 'data' folder inside my cloned 'pulse' repository.\n",
    "PULSE_DATA_BASE_PATH = r'C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data'\n",
    "\n",
    "def connect_db():\n",
    "    \"\"\"Establishes a database connection.\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        if connection.is_connected():\n",
    "            print(\"Successfully connected to MySQL database!\")\n",
    "        return connection\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error connecting to MySQL: {err}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa8c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aggregated_transaction_data():\n",
    "    \"\"\"Loads aggregated transaction data into the Aggregated_transaction table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection: return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'aggregated', 'transaction', 'country', 'india', 'state')\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    year = int(year_folder)\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            quarter = int(quarter_file.replace('.json', ''))\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                for item in data['data']['transactionData']:\n",
    "                                    transaction_type = item['name']\n",
    "                                    count = item['paymentInstruments'][0]['count']\n",
    "                                    amount = item['paymentInstruments'][0]['amount']\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO Aggregated_transaction\n",
    "                                    (state, year, quarter, transaction_type, transaction_count, transaction_amount)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        transaction_count = VALUES(transaction_count),\n",
    "                                        transaction_amount = VALUES(transaction_amount);\n",
    "                                    \"\"\"\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter, transaction_type, count, amount))\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Aggregated_transaction data. Loaded: {success_count}, Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "471976ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Aggregated_transaction data. Loaded: 5034, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_aggregated_transaction_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28938df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aggregated_user_data():\n",
    "    \"\"\"Loads aggregated user data into the Aggregated_user table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection:\n",
    "        return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    # Base path for aggregated user data\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'aggregated', 'user', 'country', 'india', 'state')\n",
    "\n",
    "    # Ensure the data path exists before proceeding\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data path not found: {data_path}\")\n",
    "        connection.close()\n",
    "        return\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            # Clean state name (remove .json if present, though it shouldn't be for folders)\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            # Corrected: Iterate over contents of state_folder_path\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    try:\n",
    "                        year = int(year_folder)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping non-integer year folder: {year_folder}\")\n",
    "                        continue\n",
    "\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            try:\n",
    "                                quarter = int(quarter_file.replace('.json', ''))\n",
    "                            except ValueError:\n",
    "                                print(f\"Skipping non-integer quarter file: {quarter_file}\")\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "\n",
    "                                # Extract data points\n",
    "                                registered_users = data['data']['aggregated']['registeredUsers']\n",
    "                                # Use .get() for 'appOpens' as it might be optional or missing\n",
    "                                app_opens = data['data']['aggregated'].get('appOpens')\n",
    "                                # Use the responseTimestamp from the JSON as the record's timestamp\n",
    "                                response_timestamp = data.get('responseTimestamp')\n",
    "\n",
    "                                # Insert query for Aggregated_user table (MySQL compatible)\n",
    "                                insert_query = \"\"\"\n",
    "                                INSERT INTO Aggregated_user\n",
    "                                (state, year, quarter, registered_users, app_opens, timestamp)\n",
    "                                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                ON DUPLICATE KEY UPDATE\n",
    "                                    registered_users = VALUES(registered_users),\n",
    "                                    app_opens = VALUES(app_opens),\n",
    "                                    timestamp = VALUES(timestamp);\n",
    "                                \"\"\"\n",
    "                                # For ON DUPLICATE KEY UPDATE to work, you need a UNIQUE constraint\n",
    "                                # or a PRIMARY KEY on (state, year, quarter) in your table definition.\n",
    "                                # Make sure your Aggregated_user table has this.\n",
    "\n",
    "                                cursor.execute(insert_query, (state_name_clean, year, quarter,\n",
    "                                                              registered_users, app_opens, response_timestamp))\n",
    "                                success_count += 1\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Missing key in JSON from {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback() # Rollback on error to maintain data integrity\n",
    "                                continue\n",
    "    connection.commit() # Commit all successful inserts\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Aggregated_user data. Loaded: {success_count}, Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b1c8d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Aggregated_user data. Loaded: 1008, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_aggregated_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ffd46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_users_by_device_data():\n",
    "    \"\"\"Loads users by device data into the users_by_device table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection:\n",
    "        return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    # Base path for aggregated user data (same as for aggregated_user_data)\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'aggregated', 'user', 'country', 'india', 'state')\n",
    "\n",
    "    # Ensure the data path exists before proceeding\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data path not found: {data_path}\")\n",
    "        connection.close()\n",
    "        return\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            # Corrected: Iterate over contents of state_folder_path\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    try:\n",
    "                        year = int(year_folder)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping non-integer year folder: {year_folder}\")\n",
    "                        continue\n",
    "\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            try:\n",
    "                                quarter = int(quarter_file.replace('.json', ''))\n",
    "                            except ValueError:\n",
    "                                print(f\"Skipping non-integer quarter file: {quarter_file}\")\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "\n",
    "                                # Extract usersByDevice array\n",
    "                                # Ensure data['data'] is not None before calling .get() on it\n",
    "                                # And ensure usersByDevice_list is an empty list if its value is None\n",
    "                                users_by_device_list = []\n",
    "                                if 'data' in data and data['data'] is not None:\n",
    "                                    users_by_device_list = data['data'].get('usersByDevice')\n",
    "                                    if users_by_device_list is None:\n",
    "                                        users_by_device_list = [] # Explicitly set to empty list if null\n",
    "\n",
    "                                response_timestamp = data.get('responseTimestamp')\n",
    "\n",
    "                                for device_data in users_by_device_list: # This loop expects an iterable\n",
    "                                    brand_name = device_data.get('brand')\n",
    "                                    count = device_data.get('count')\n",
    "                                    percentage = device_data.get('percentage')\n",
    "\n",
    "                                    if brand_name is None or count is None or percentage is None:\n",
    "                                        print(f\"Skipping incomplete device data in {quarter_path}: {device_data}\")\n",
    "                                        continue\n",
    "\n",
    "                                    # Insert query for users_by_device table (MySQL compatible)\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO users_by_device\n",
    "                                    (state, year, quarter, brand_name, count, percentage, timestamp)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        count = VALUES(count),\n",
    "                                        percentage = VALUES(percentage),\n",
    "                                        timestamp = VALUES(timestamp);\n",
    "                                    \"\"\"\n",
    "                                    # For ON DUPLICATE KEY UPDATE to work, you need a UNIQUE constraint\n",
    "                                    # or a PRIMARY KEY on (state, year, quarter, brand_name) in your table definition.\n",
    "\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter,\n",
    "                                                                  brand_name, count, percentage, response_timestamp))\n",
    "                                    success_count += 1\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Missing key in JSON from {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading users_by_device data. Loaded: {success_count}, Errors: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2550e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading users_by_device data. Loaded: 6732, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_users_by_device_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd49c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aggregated_insurance_data():\n",
    "    \"\"\"Loads aggregated insurance data into the Aggregated_insurance table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection: return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'aggregated', 'insurance', 'country', 'india', 'state')\n",
    "\n",
    "    for state_name_folder in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name_folder)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name_folder.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    year = int(year_folder)\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            file_full_path = os.path.join(year_path, quarter_file)\n",
    "                            quarter = int(quarter_file.replace('.json', ''))\n",
    "                            try:\n",
    "                                with open(file_full_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                for item in data['data']['transactionData']: # Structure is similar to aggregated_transaction\n",
    "                                    insurance_type = item['name']\n",
    "                                    premium_count = item['paymentInstruments'][0]['count']\n",
    "                                    premium_amount = item['paymentInstruments'][0]['amount']\n",
    "\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO Aggregated_insurance\n",
    "                                    (state, year, quarter, insurance_type, premium_count, premium_amount)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        premium_count = VALUES(premium_count),\n",
    "                                        premium_amount = VALUES(premium_amount);\n",
    "                                    \"\"\"\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter, insurance_type, premium_count, premium_amount))\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {file_full_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Aggregated_insurance data. Loaded: {success_count}, Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92af475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Aggregated_insurance data. Loaded: 682, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_aggregated_insurance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8caa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map_transaction_data():\n",
    "    \"\"\"Loads map transaction data into the Map_transaction table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection: return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'map', 'transaction', 'hover', 'country', 'india', 'state')\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    year = int(year_folder)\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            quarter = int(quarter_file.replace('.json', ''))\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                for district_data in data['data']['hoverDataList']:\n",
    "                                    district_name = district_data['name']\n",
    "                                    # Assuming there's only one paymentInstrument for total count/amount for the district\n",
    "                                    transaction_count = district_data['metric'][0]['count']\n",
    "                                    transaction_amount = district_data['metric'][0]['amount']\n",
    "\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO Map_transaction\n",
    "                                    (state, year, quarter, district, transaction_count, transaction_amount)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        transaction_count = VALUES(transaction_count),\n",
    "                                        transaction_amount = VALUES(transaction_amount);\n",
    "                                    \"\"\"\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter, district_name, transaction_count, transaction_amount))\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Map_transaction data. Loaded: {success_count}, Errors: {error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063805ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Map_transaction data. Loaded: 20604, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_map_transaction_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a48a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map_user_data():\n",
    "    \"\"\"Loads map user data into the Map_user table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection: return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'map', 'user', 'hover', 'country', 'india', 'state')\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    year = int(year_folder)\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            quarter = int(quarter_file.replace('.json', ''))\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                # *** CHANGE STARTS HERE ***\n",
    "                                # Iterate through items in the 'hoverData' dictionary\n",
    "                                for district_name, user_data in data['data']['hoverData'].items():\n",
    "                                    registered_users = user_data['registeredUsers']\n",
    "                                    app_opens = user_data.get('appOpens') # Use .get() for appOpens as it might be optional\n",
    "                                # *** CHANGE ENDS HERE ***\n",
    "\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO Map_user\n",
    "                                    (state, year, quarter, district, registered_users, app_opens)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        registered_users = VALUES(registered_users),\n",
    "                                        app_opens = VALUES(app_opens);\n",
    "                                    \"\"\"\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter, district_name, registered_users, app_opens))\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Map_user data. Loaded: {success_count}, Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dde896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Map_user data. Loaded: 20608, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_map_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d0a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_map_insurance_data():\n",
    "    \"\"\"Loads map insurance data into the Map_insurance table.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection: return\n",
    "    cursor = connection.cursor()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'map', 'insurance', 'hover', 'country', 'india', 'state')\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    year = int(year_folder)\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            quarter = int(quarter_file.replace('.json', ''))\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "                                for district_data in data['data']['hoverDataList']:\n",
    "                                    district_name = district_data['name']\n",
    "                                    # Assuming there's only one paymentInstrument for total count/amount for the district\n",
    "                                    premium_count = district_data['metric'][0]['count']\n",
    "                                    premium_amount = district_data['metric'][0]['amount']\n",
    "\n",
    "                                    insert_query = \"\"\"\n",
    "                                    INSERT INTO Map_insurance\n",
    "                                    (state, year, quarter, district, premium_count, premium_amount)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        premium_count = VALUES(premium_count),\n",
    "                                        premium_amount = VALUES(premium_amount);\n",
    "                                    \"\"\"\n",
    "                                    cursor.execute(insert_query, (state_name_clean, year, quarter, district_name, premium_count, premium_amount))\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Map_insurance data. Loaded: {success_count}, Errors: {error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d930c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Map_insurance data. Loaded: 13876, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_map_insurance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27fb50a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_transaction_data():\n",
    "    \"\"\"Loads top transaction data (districts and pincodes) into respective tables.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection:\n",
    "        return\n",
    "    cursor = connection.cursor()\n",
    "    districts_success_count = 0\n",
    "    districts_error_count = 0\n",
    "    pincodes_success_count = 0\n",
    "    pincodes_error_count = 0\n",
    "\n",
    "    # Base path for top transaction data (adjust if your path is different)\n",
    "    # Assuming path like: PULSE_DATA_BASE_PATH/top/insurance/country/india/state/{state}/{year}/{quarter}.json\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'top', 'transaction', 'country', 'india', 'state')\n",
    "\n",
    "    # Ensure the data path exists before proceeding\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data path not found: {data_path}\")\n",
    "        connection.close()\n",
    "        return\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    try:\n",
    "                        year = int(year_folder)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping non-integer year folder: {year_folder}\")\n",
    "                        continue\n",
    "\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            try:\n",
    "                                quarter = int(quarter_file.replace('.json', ''))\n",
    "                            except ValueError:\n",
    "                                print(f\"Skipping non-integer quarter file: {quarter_file}\")\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "\n",
    "                                response_timestamp = data.get('responseTimestamp')\n",
    "\n",
    "                                # --- Process Districts Data ---\n",
    "                                districts_list = data['data'].get('districts')\n",
    "                                if districts_list is None: # Handle case where 'districts' is null\n",
    "                                    districts_list = []\n",
    "\n",
    "                                for district_data in districts_list:\n",
    "                                    entity_name = district_data.get('entityName')\n",
    "                                    metric = district_data.get('metric')\n",
    "                                    if entity_name and metric:\n",
    "                                        metric_type = metric.get('type')\n",
    "                                        count = metric.get('count')\n",
    "                                        amount = metric.get('amount')\n",
    "\n",
    "                                        if all(v is not None for v in [metric_type, count, amount]):\n",
    "                                            insert_query_districts = \"\"\"\n",
    "                                            INSERT INTO top_transaction_districts_data\n",
    "                                            (state, year, quarter, district_name, metric_type, count, amount, timestamp)\n",
    "                                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                            ON DUPLICATE KEY UPDATE\n",
    "                                                metric_type = VALUES(metric_type),\n",
    "                                                count = VALUES(count),\n",
    "                                                amount = VALUES(amount),\n",
    "                                                timestamp = VALUES(timestamp);\n",
    "                                            \"\"\"\n",
    "                                            try:\n",
    "                                                cursor.execute(insert_query_districts, (state_name_clean, year, quarter,\n",
    "                                                                                        entity_name, metric_type, count, amount,\n",
    "                                                                                        response_timestamp))\n",
    "                                                districts_success_count += 1\n",
    "                                            except Exception as e:\n",
    "                                                print(f\"Error inserting district data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                                districts_error_count += 1\n",
    "                                                connection.rollback()\n",
    "                                        else:\n",
    "                                            print(f\"Skipping incomplete district metric data in {quarter_path}: {district_data}\")\n",
    "                                    else:\n",
    "                                        print(f\"Skipping incomplete district data in {quarter_path}: {district_data}\")\n",
    "\n",
    "                                # --- Process Pincodes Data ---\n",
    "                                pincodes_list = data['data'].get('pincodes')\n",
    "                                if pincodes_list is None: # Handle case where 'pincodes' is null\n",
    "                                    pincodes_list = []\n",
    "\n",
    "                                for pincode_data in pincodes_list:\n",
    "                                    entity_name = pincode_data.get('entityName')\n",
    "                                    metric = pincode_data.get('metric')\n",
    "                                    if entity_name and metric:\n",
    "                                        metric_type = metric.get('type')\n",
    "                                        count = metric.get('count')\n",
    "                                        amount = metric.get('amount')\n",
    "\n",
    "                                        if all(v is not None for v in [metric_type, count, amount]):\n",
    "                                            insert_query_pincodes = \"\"\"\n",
    "                                            INSERT INTO top_transaction_pincodes_data\n",
    "                                            (state, year, quarter, pincode, metric_type, count, amount, timestamp)\n",
    "                                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                            ON DUPLICATE KEY UPDATE\n",
    "                                                metric_type = VALUES(metric_type),\n",
    "                                                count = VALUES(count),\n",
    "                                                amount = VALUES(amount),\n",
    "                                                timestamp = VALUES(timestamp);\n",
    "                                            \"\"\"\n",
    "                                            try:\n",
    "                                                cursor.execute(insert_query_pincodes, (state_name_clean, year, quarter,\n",
    "                                                                                       entity_name, metric_type, count, amount,\n",
    "                                                                                       response_timestamp))\n",
    "                                                pincodes_success_count += 1\n",
    "                                            except Exception as e:\n",
    "                                                print(f\"Error inserting pincode data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                                pincodes_error_count += 1\n",
    "                                                connection.rollback()\n",
    "                                        else:\n",
    "                                            print(f\"Skipping incomplete pincode metric data in {quarter_path}: {pincode_data}\")\n",
    "                                    else:\n",
    "                                        print(f\"Skipping incomplete pincode data in {quarter_path}: {pincode_data}\")\n",
    "\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1 # Count as error for file\n",
    "                                pincodes_error_count += 1 # Count as error for file\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Missing key in JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1\n",
    "                                pincodes_error_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1\n",
    "                                pincodes_error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Top Insurance data.\")\n",
    "    print(f\"  Districts: Loaded: {districts_success_count}, Errors: {districts_error_count}\")\n",
    "    print(f\"  Pincodes: Loaded: {pincodes_success_count}, Errors: {pincodes_error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae2080ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "Skipping incomplete pincode data in C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data\\top\\transaction\\country\\india\\state\\ladakh\\2019\\4.json: {'entityName': None, 'metric': {'type': 'TOTAL', 'count': 2014, 'amount': 10098656.16799601}}\n",
      "Skipping incomplete pincode data in C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data\\top\\transaction\\country\\india\\state\\ladakh\\2020\\4.json: {'entityName': None, 'metric': {'type': 'TOTAL', 'count': 13717, 'amount': 36711603.92132313}}\n",
      "\n",
      "Finished loading Top Insurance data.\n",
      "  Districts: Loaded: 8296, Errors: 0\n",
      "  Pincodes: Loaded: 9997, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_top_transaction_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b0a0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_user_data():\n",
    "    \"\"\"Loads top user data (districts and pincodes) into respective tables.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection:\n",
    "        return\n",
    "    cursor = connection.cursor()\n",
    "    districts_success_count = 0\n",
    "    districts_error_count = 0\n",
    "    pincodes_success_count = 0\n",
    "    pincodes_error_count = 0\n",
    "\n",
    "    # Base path for top user data (adjust if your path is different)\n",
    "    # Assuming path like: PULSE_DATA_BASE_PATH/top/user/country/india/state/{state}/{year}/{quarter}.json\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'top', 'user', 'country', 'india', 'state')\n",
    "\n",
    "    # Ensure the data path exists before proceeding\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data path not found: {data_path}\")\n",
    "        connection.close()\n",
    "        return\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    try:\n",
    "                        year = int(year_folder)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping non-integer year folder: {year_folder}\")\n",
    "                        continue\n",
    "\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            try:\n",
    "                                quarter = int(quarter_file.replace('.json', ''))\n",
    "                            except ValueError:\n",
    "                                print(f\"Skipping non-integer quarter file: {quarter_file}\")\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "\n",
    "                                response_timestamp = data.get('responseTimestamp')\n",
    "\n",
    "                                # --- Process Districts Data ---\n",
    "                                districts_list = data['data'].get('districts')\n",
    "                                if districts_list is None: # Handle case where 'districts' is null\n",
    "                                    districts_list = []\n",
    "\n",
    "                                for district_data in districts_list:\n",
    "                                    entity_name = district_data.get('name')\n",
    "                                    registered_users = district_data.get('registeredUsers')\n",
    "\n",
    "                                    # More explicit checks and error messages\n",
    "                                    if entity_name is None:\n",
    "                                        print(f\"Skipping district data from {quarter_path}: 'name' is missing or None in {district_data}\")\n",
    "                                        districts_error_count += 1\n",
    "                                        continue\n",
    "                                    if registered_users is None:\n",
    "                                        print(f\"Skipping district data from {quarter_path}: 'registeredUsers' is missing or None in {district_data}\")\n",
    "                                        districts_error_count += 1\n",
    "                                        continue\n",
    "\n",
    "                                    insert_query_districts = \"\"\"\n",
    "                                    INSERT INTO top_user_districts_data\n",
    "                                    (state, year, quarter, district_name, registered_users, timestamp)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        registered_users = VALUES(registered_users),\n",
    "                                        timestamp = VALUES(timestamp);\n",
    "                                    \"\"\"\n",
    "                                    try:\n",
    "                                        cursor.execute(insert_query_districts, (state_name_clean, year, quarter,\n",
    "                                                                                entity_name, registered_users,\n",
    "                                                                                response_timestamp))\n",
    "                                        districts_success_count += 1\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error inserting district data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                        districts_error_count += 1\n",
    "                                        connection.rollback()\n",
    "\n",
    "                                # --- Process Pincodes Data ---\n",
    "                                pincodes_list = data['data'].get('pincodes')\n",
    "                                if pincodes_list is None: # Handle case where 'pincodes' is null\n",
    "                                    pincodes_list = []\n",
    "\n",
    "                                for pincode_data in pincodes_list:\n",
    "                                    entity_name = pincode_data.get('name')\n",
    "                                    registered_users = pincode_data.get('registeredUsers')\n",
    "\n",
    "                                    # More explicit checks and error messages\n",
    "                                    if entity_name is None:\n",
    "                                        print(f\"Skipping pincode data from {quarter_path}: 'name' is missing or None in {pincode_data}\")\n",
    "                                        pincodes_error_count += 1\n",
    "                                        continue\n",
    "                                    if registered_users is None:\n",
    "                                        print(f\"Skipping pincode data from {quarter_path}: 'registeredUsers' is missing or None in {pincode_data}\")\n",
    "                                        pincodes_error_count += 1\n",
    "                                        continue\n",
    "\n",
    "                                    insert_query_pincodes = \"\"\"\n",
    "                                    INSERT INTO top_user_pincodes_data\n",
    "                                    (state, year, quarter, pincode, registered_users, timestamp)\n",
    "                                    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                                    ON DUPLICATE KEY UPDATE\n",
    "                                        registered_users = VALUES(registered_users),\n",
    "                                        timestamp = VALUES(timestamp);\n",
    "                                    \"\"\"\n",
    "                                    try:\n",
    "                                        cursor.execute(insert_query_pincodes, (state_name_clean, year, quarter,\n",
    "                                                                               entity_name, registered_users,\n",
    "                                                                               response_timestamp))\n",
    "                                        pincodes_success_count += 1\n",
    "                                    except Exception as e:\n",
    "                                        print(f\"Error inserting pincode data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                        pincodes_error_count += 1\n",
    "                                        connection.rollback()\n",
    "\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1 # Count as error for file\n",
    "                                pincodes_error_count += 1 # Count as error for file\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Missing expected key in JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1\n",
    "                                pincodes_error_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                error_count += 1 # General error count for the file\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Top User data.\")\n",
    "    print(f\"  Districts: Loaded: {districts_success_count}, Errors: {districts_error_count}\")\n",
    "    print(f\"  Pincodes: Loaded: {pincodes_success_count}, Errors: {pincodes_error_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc274cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "\n",
      "Finished loading Top User data.\n",
      "  Districts: Loaded: 8296, Errors: 0\n",
      "  Pincodes: Loaded: 10000, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_top_user_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "de1b5950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_top_insurance_data():\n",
    "    \"\"\"Loads top transaction data (districts and pincodes) into respective tables.\"\"\"\n",
    "    connection = connect_db()\n",
    "    if not connection:\n",
    "        return\n",
    "    cursor = connection.cursor()\n",
    "    districts_success_count = 0\n",
    "    districts_error_count = 0\n",
    "    pincodes_success_count = 0\n",
    "    pincodes_error_count = 0\n",
    "\n",
    "    # Base path for top transaction data (adjust if your path is different)\n",
    "    data_path = os.path.join(PULSE_DATA_BASE_PATH, 'top', 'insurance', 'country', 'india', 'state')\n",
    "\n",
    "    # Ensure the data path exists before proceeding\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Error: Data path not found: {data_path}\")\n",
    "        connection.close()\n",
    "        return\n",
    "\n",
    "    for state_name in os.listdir(data_path):\n",
    "        state_folder_path = os.path.join(data_path, state_name)\n",
    "        if os.path.isdir(state_folder_path):\n",
    "            state_name_clean = state_name.replace('.json', '')\n",
    "            for year_folder in os.listdir(state_folder_path):\n",
    "                year_path = os.path.join(state_folder_path, year_folder)\n",
    "                if os.path.isdir(year_path):\n",
    "                    try:\n",
    "                        year = int(year_folder)\n",
    "                    except ValueError:\n",
    "                        print(f\"Skipping non-integer year folder: {year_folder}\")\n",
    "                        continue\n",
    "\n",
    "                    for quarter_file in os.listdir(year_path):\n",
    "                        if quarter_file.endswith('.json'):\n",
    "                            quarter_path = os.path.join(year_path, quarter_file)\n",
    "                            try:\n",
    "                                quarter = int(quarter_file.replace('.json', ''))\n",
    "                            except ValueError:\n",
    "                                print(f\"Skipping non-integer quarter file: {quarter_file}\")\n",
    "                                continue\n",
    "\n",
    "                            try:\n",
    "                                with open(quarter_path, 'r') as f:\n",
    "                                    data = json.load(f)\n",
    "\n",
    "                                response_timestamp = data.get('responseTimestamp')\n",
    "\n",
    "                                # --- Process Districts Data ---\n",
    "                                districts_list = data['data'].get('districts')\n",
    "                                if districts_list is None: # Handle case where 'districts' is null\n",
    "                                    districts_list = []\n",
    "\n",
    "                                for district_data in districts_list:\n",
    "                                    entity_name = district_data.get('entityName')\n",
    "                                    metric = district_data.get('metric')\n",
    "                                    if entity_name and metric:\n",
    "                                        metric_type = metric.get('type')\n",
    "                                        count = metric.get('count')\n",
    "                                        amount = metric.get('amount')\n",
    "\n",
    "                                        if all(v is not None for v in [metric_type, count, amount]):\n",
    "                                            insert_query_districts = \"\"\"\n",
    "                                            INSERT INTO top_insurance_districts_data\n",
    "                                            (state, year, quarter, district_name, metric_type, count, amount, timestamp)\n",
    "                                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                            ON DUPLICATE KEY UPDATE\n",
    "                                                metric_type = VALUES(metric_type),\n",
    "                                                count = VALUES(count),\n",
    "                                                amount = VALUES(amount),\n",
    "                                                timestamp = VALUES(timestamp);\n",
    "                                            \"\"\"\n",
    "                                            try:\n",
    "                                                cursor.execute(insert_query_districts, (state_name_clean, year, quarter,\n",
    "                                                                                        entity_name, metric_type, count, amount,\n",
    "                                                                                        response_timestamp))\n",
    "                                                districts_success_count += 1\n",
    "                                            except Exception as e:\n",
    "                                                print(f\"Error inserting district data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                                districts_error_count += 1\n",
    "                                                connection.rollback()\n",
    "                                        else:\n",
    "                                            print(f\"Skipping incomplete district metric data in {quarter_path}: {district_data}\")\n",
    "                                    else:\n",
    "                                        print(f\"Skipping incomplete district data in {quarter_path}: {district_data}\")\n",
    "\n",
    "                                # --- Process Pincodes Data ---\n",
    "                                pincodes_list = data['data'].get('pincodes')\n",
    "                                if pincodes_list is None: # Handle case where 'pincodes' is null\n",
    "                                    pincodes_list = []\n",
    "\n",
    "                                for pincode_data in pincodes_list:\n",
    "                                    entity_name = pincode_data.get('entityName')\n",
    "                                    metric = pincode_data.get('metric')\n",
    "                                    if entity_name and metric:\n",
    "                                        metric_type = metric.get('type')\n",
    "                                        count = metric.get('count')\n",
    "                                        amount = metric.get('amount')\n",
    "\n",
    "                                        if all(v is not None for v in [metric_type, count, amount]):\n",
    "                                            insert_query_pincodes = \"\"\"\n",
    "                                            INSERT INTO top_insurance_pincodes_data\n",
    "                                            (state, year, quarter, pincode, metric_type, count, amount, timestamp)\n",
    "                                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                                            ON DUPLICATE KEY UPDATE\n",
    "                                                metric_type = VALUES(metric_type),\n",
    "                                                count = VALUES(count),\n",
    "                                                amount = VALUES(amount),\n",
    "                                                timestamp = VALUES(timestamp);\n",
    "                                            \"\"\"\n",
    "                                            try:\n",
    "                                                cursor.execute(insert_query_pincodes, (state_name_clean, year, quarter,\n",
    "                                                                                       entity_name, metric_type, count, amount,\n",
    "                                                                                       response_timestamp))\n",
    "                                                pincodes_success_count += 1\n",
    "                                            except Exception as e:\n",
    "                                                print(f\"Error inserting pincode data from {quarter_path} ({entity_name}): {e}\")\n",
    "                                                pincodes_error_count += 1\n",
    "                                                connection.rollback()\n",
    "                                        else:\n",
    "                                            print(f\"Skipping incomplete pincode metric data in {quarter_path}: {pincode_data}\")\n",
    "                                    else:\n",
    "                                        print(f\"Skipping incomplete pincode data in {quarter_path}: {pincode_data}\")\n",
    "\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1 # Count as error for file\n",
    "                                pincodes_error_count += 1 # Count as error for file\n",
    "                            except KeyError as e:\n",
    "                                print(f\"Missing key in JSON from {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1\n",
    "                                pincodes_error_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error processing {quarter_path}: {e}\")\n",
    "                                districts_error_count += 1\n",
    "                                pincodes_error_count += 1\n",
    "                                connection.rollback()\n",
    "                                continue\n",
    "    connection.commit()\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    print(f\"\\nFinished loading Top Transaction data.\")\n",
    "    print(f\"  Districts: Loaded: {districts_success_count}, Errors: {districts_error_count}\")\n",
    "    print(f\"  Pincodes: Loaded: {pincodes_success_count}, Errors: {pincodes_error_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2a9c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database!\n",
      "Skipping incomplete pincode data in C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data\\top\\insurance\\country\\india\\state\\ladakh\\2020\\3.json: {'entityName': None, 'metric': {'type': 'TOTAL', 'count': 1, 'amount': 281.0}}\n",
      "Skipping incomplete pincode data in C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data\\top\\insurance\\country\\india\\state\\ladakh\\2020\\4.json: {'entityName': None, 'metric': {'type': 'TOTAL', 'count': 1, 'amount': 658.0}}\n",
      "Skipping incomplete pincode data in C:\\Users\\hanum\\OneDrive\\Desktop\\Labmentix\\week6\\data\\top\\insurance\\country\\india\\state\\ladakh\\2022\\4.json: {'entityName': None, 'metric': {'type': 'TOTAL', 'count': 8, 'amount': 16020.0}}\n",
      "\n",
      "Finished loading Top Transaction data.\n",
      "  Districts: Loaded: 5608, Errors: 0\n",
      "  Pincodes: Loaded: 6665, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "load_top_insurance_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac009f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_table_to_xlsx(table_name, output_directory=\"excel_exports\"):\n",
    "    \n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(**DB_CONFIG)\n",
    "        if connection.is_connected():\n",
    "            print(f\"Connected to MySQL to export '{table_name}'...\")\n",
    "            \n",
    "            # Create the output directory if it doesn't exist\n",
    "            if not os.path.exists(output_directory):\n",
    "                os.makedirs(output_directory)\n",
    "                print(f\"Created directory: {output_directory}\")\n",
    "\n",
    "            # Construct the SQL query to select all data from the table\n",
    "            query = f\"SELECT * FROM {table_name};\"\n",
    "            \n",
    "            cursor = connection.cursor(dictionary=True) # Fetch as dictionaries for pandas\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "\n",
    "            if results:\n",
    "                df = pd.DataFrame(results)\n",
    "                output_filepath = os.path.join(output_directory, f\"{table_name}.xlsx\")\n",
    "                # index=False prevents writing the DataFrame index as a column\n",
    "                # sheet_name sets the name of the first (and only) sheet in the Excel file\n",
    "                df.to_excel(output_filepath, index=False, sheet_name=table_name) \n",
    "                print(f\"Successfully exported data from '{table_name}' to '{output_filepath}'\")\n",
    "            else:\n",
    "                print(f\"No data found in table '{table_name}'. No Excel file created.\")\n",
    "\n",
    "    except mysql.connector.Error as err:\n",
    "        print(f\"Error exporting table '{table_name}': {err}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while exporting '{table_name}': {e}\")\n",
    "    finally:\n",
    "        if connection and connection.is_connected():\n",
    "            if 'cursor' in locals() and cursor is not None:\n",
    "                cursor.close()\n",
    "            connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ce6cd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL to export 'Aggregated_transaction'...\n",
      "Successfully exported data from 'Aggregated_transaction' to 'excel_exports\\Aggregated_transaction.xlsx'\n",
      "Connected to MySQL to export 'Aggregated_user'...\n",
      "Successfully exported data from 'Aggregated_user' to 'excel_exports\\Aggregated_user.xlsx'\n",
      "Connected to MySQL to export 'Aggregated_insurance'...\n",
      "Successfully exported data from 'Aggregated_insurance' to 'excel_exports\\Aggregated_insurance.xlsx'\n",
      "Connected to MySQL to export 'Map_transaction'...\n",
      "Successfully exported data from 'Map_transaction' to 'excel_exports\\Map_transaction.xlsx'\n",
      "Connected to MySQL to export 'Map_user'...\n",
      "Successfully exported data from 'Map_user' to 'excel_exports\\Map_user.xlsx'\n",
      "Connected to MySQL to export 'Map_insurance'...\n",
      "Successfully exported data from 'Map_insurance' to 'excel_exports\\Map_insurance.xlsx'\n",
      "Connected to MySQL to export 'top_insurance_districts_data'...\n",
      "Successfully exported data from 'top_insurance_districts_data' to 'excel_exports\\top_insurance_districts_data.xlsx'\n",
      "Connected to MySQL to export 'top_insurance_pincodes_data'...\n",
      "Successfully exported data from 'top_insurance_pincodes_data' to 'excel_exports\\top_insurance_pincodes_data.xlsx'\n",
      "Connected to MySQL to export 'top_transaction_districts_data'...\n",
      "Successfully exported data from 'top_transaction_districts_data' to 'excel_exports\\top_transaction_districts_data.xlsx'\n",
      "Connected to MySQL to export 'top_transaction_pincodes_data'...\n",
      "Successfully exported data from 'top_transaction_pincodes_data' to 'excel_exports\\top_transaction_pincodes_data.xlsx'\n",
      "Connected to MySQL to export 'top_user_districts_data'...\n",
      "Successfully exported data from 'top_user_districts_data' to 'excel_exports\\top_user_districts_data.xlsx'\n",
      "Connected to MySQL to export 'top_user_pincodes_data'...\n",
      "Successfully exported data from 'top_user_pincodes_data' to 'excel_exports\\top_user_pincodes_data.xlsx'\n",
      "Connected to MySQL to export 'users_by_device'...\n",
      "Successfully exported data from 'users_by_device' to 'excel_exports\\users_by_device.xlsx'\n",
      "\n",
      "Excel (.xlsx) export process completed for all specified tables.\n"
     ]
    }
   ],
   "source": [
    "tables_to_export = [\n",
    "    \"Aggregated_transaction\",\n",
    "    \"Aggregated_user\",\n",
    "    \"Aggregated_insurance\",\n",
    "    \"Map_transaction\",\n",
    "    \"Map_user\",\n",
    "    \"Map_insurance\",\n",
    "    \"top_insurance_districts_data\",\n",
    "    \"top_insurance_pincodes_data\",\n",
    "    \"top_transaction_districts_data\",\n",
    "    \"top_transaction_pincodes_data\",\n",
    "    \"top_user_districts_data\",\n",
    "    \"top_user_pincodes_data\",\n",
    "    \"users_by_device\"\n",
    "]\n",
    "\n",
    "# --- Execute export for each table ---\n",
    "if __name__ == \"__main__\":\n",
    "    for table in tables_to_export:\n",
    "        export_table_to_xlsx(table)\n",
    "    print(\"\\nExcel (.xlsx) export process completed for all specified tables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
